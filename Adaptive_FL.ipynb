{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nJNFoHiJdFS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle, resample\n",
        "\n",
        "\n",
        "model_storage = \"/content/i_models\"\n",
        "data_dir = \"/content/preprocessed_data\"\n",
        "\n",
        "\n",
        "iterations = 2\n",
        "epochs_per_iteration = 2\n",
        "batch_size = 32\n",
        "global_model_type = \"gru\"\n",
        "\n",
        "global_features_file = os.path.join(data_dir, \"global_features.json\")\n",
        "scaler_and_encoders_file = os.path.join(data_dir, \"scalers_and_encoders.pkl\")\n",
        "\n",
        "with open(global_features_file, \"r\") as f:\n",
        "    global_features = json.load(f)\n",
        "\n",
        "with open(scaler_and_encoders_file, \"rb\") as f:\n",
        "    preprocessors = pickle.load(f)\n",
        "\n",
        "global_scaler = preprocessors[\"scaler\"]\n",
        "label_encoders = preprocessors[\"encoders\"]\n",
        "\n",
        "def build_model(model_type, input_shape):\n",
        "    if model_type == \"lstm\":\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "    elif model_type == \"gru\":\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.GRU(64, input_shape=input_shape, return_sequences=True),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.GRU(32, return_sequences=False),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type\")\n",
        "\n",
        "def augment_data(X, y):\n",
        "    noise = 0.01 * np.random.normal(size=X.shape)\n",
        "    X_augmented = X + noise\n",
        "    return np.vstack([X, X_augmented]), np.hstack([y, y])\n",
        "\n",
        "def resample_data(X, y):\n",
        "    X_reshaped = X.reshape(X.shape[0], -1)\n",
        "    data = pd.concat([pd.DataFrame(X_reshaped), pd.Series(y, name=\"label\")], axis=1)\n",
        "\n",
        "    minority = data[data[\"label\"] == 1]\n",
        "    majority = data[data[\"label\"] == 0]\n",
        "    minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
        "    data_resampled = pd.concat([majority, minority_upsampled])\n",
        "    X_resampled = data_resampled.iloc[:, :-1].values\n",
        "    y_resampled = data_resampled[\"label\"].values\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 2:\n",
        "        return lr\n",
        "    return lr * 0.8\n",
        "\n",
        "def train_local_model(local_model, X_train, y_train, X_val, y_val, model_type):\n",
        "    learning_rate = 0.001\n",
        "    local_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                        loss=\"binary_crossentropy\",\n",
        "                        metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=3, restore_best_weights=True, verbose=1\n",
        "    )\n",
        "\n",
        "    local_model.fit(X_train, y_train,\n",
        "                    epochs=epochs_per_iteration,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[lr_scheduler, early_stopping],\n",
        "                    verbose=1)\n",
        "\n",
        "    return local_model\n",
        "\n",
        "def federated_averaging(models, sample_counts):\n",
        "    new_weights = []\n",
        "    for weights_list_tuple in zip(*[model.get_weights() for model in models]):\n",
        "        new_weights.append(np.average(weights_list_tuple, axis=0, weights=sample_counts))\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "def federated_training(data_dir):\n",
        "\n",
        "    input_shape = (len(global_features), 1)\n",
        "    global_model = build_model(global_model_type, input_shape)\n",
        "\n",
        "\n",
        "    global_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")],\n",
        "    )\n",
        "\n",
        "\n",
        "    local_data = []\n",
        "    local_models = []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.startswith(\"preprocessed_\") and filename.endswith(\".csv\"):\n",
        "            data_path = os.path.join(data_dir, filename)\n",
        "\n",
        "\n",
        "            data = pd.read_csv(data_path)\n",
        "\n",
        "\n",
        "            for col in global_features:\n",
        "                if col not in data.columns:\n",
        "                    data[col] = 0.0\n",
        "\n",
        "            X = data[global_features]\n",
        "            y = data[\"label\"]\n",
        "\n",
        "            X_scaled = global_scaler.transform(X)\n",
        "\n",
        "            X_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "            X_shuffled, y_shuffled = shuffle(X_scaled, y, random_state=42)\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X_shuffled, y_shuffled, test_size=0.2, random_state=42)\n",
        "\n",
        "            local_data.append((X_train, X_val, y_train, y_val))\n",
        "\n",
        "            local_model = build_model(global_model_type, X_train.shape[1:])\n",
        "            local_models.append(local_model)\n",
        "\n",
        "    num_rounds = 2\n",
        "    global_results = []\n",
        "    for round_num in range(num_rounds):\n",
        "        print(f\"\\n--- Federated Learning Round {round_num + 1}/{num_rounds} ---\")\n",
        "        local_weights = []\n",
        "        sample_counts = []\n",
        "\n",
        "        for client, (X_train, X_val, y_train, y_val) in enumerate(local_data):\n",
        "            print(f\"\\nTraining for client {client + 1}/{len(local_data)}\")\n",
        "\n",
        "            X_train_aug, y_train_aug = augment_data(X_train, y_train)\n",
        "            X_train_res, y_train_res = resample_data(X_train_aug, y_train_aug)\n",
        "\n",
        "            X_train_res = X_train_res.reshape(X_train_res.shape[0], X_train_res.shape[1], 1)\n",
        "\n",
        "            local_model = local_models[client]\n",
        "            local_model = train_local_model(local_model, X_train_res, y_train_res, X_val, y_val, global_model_type)\n",
        "\n",
        "            local_weights.append(local_model.get_weights())\n",
        "            sample_counts.append(len(y_train_res))\n",
        "\n",
        "        aggregated_weights = federated_averaging(local_models, sample_counts)\n",
        "        global_model.set_weights(aggregated_weights)\n",
        "\n",
        "        X_val_all = np.vstack([X_val for _, X_val, _, _ in local_data])\n",
        "        y_val_all = np.hstack([y_val for _, X_val, _, y_val in local_data])\n",
        "\n",
        "        global_loss, global_accuracy, global_auc = global_model.evaluate(X_val_all, y_val_all, verbose=0)\n",
        "        print(f\"Global Validation - Round {round_num + 1}: Loss = {global_loss:.4f}, \"\n",
        "              f\"Accuracy = {global_accuracy:.4f}, AUC = {global_auc:.4f}\")\n",
        "        global_results.append((global_loss, global_accuracy, global_auc))\n",
        "\n",
        "    print(\"\\nFederated Learning Training Completed!\")\n",
        "    print(\"\\n--- Global Model Results Over Rounds ---\")\n",
        "    for round_num, (loss, accuracy, auc) in enumerate(global_results, start=1):\n",
        "        print(f\"Round {round_num}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}, AUC = {auc:.4f}\")\n",
        "\n",
        "    return global_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    federated_training(data_dir)"
      ]
    }
  ]
}